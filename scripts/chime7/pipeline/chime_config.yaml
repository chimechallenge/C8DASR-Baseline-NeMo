
name: &name "chime8-notsofar"

chime_data_root: /media/data2/chime7-challenge/datasets/chime7_official_cleaned_v2

vad_model_path: /media/data2/chime7-challenge/checkpoints/frame_vad_chime7_acrobat.nemo
msdd_model_path: /media/data2/chime7-challenge/checkpoints/msdd_v2_PALO_bs6_a003_version6_e53.ckpt 
asr_model_path: /media/data2/chime7-challenge/checkpoints/rno_chime7_chime6_ft_ptDataSetasrset3_frontend_nemoGSSv1_prec32_layers24_heads8_conv5_d1024_dlayers2_dsize640_bs128_adamw_CosineAnnealing_lr0.0001_wd1e-2_spunigram1024.nemo
lm_model_path: /media/data2/chime7-challenge/checkpoints/7gram_0001.kenlm

scenarios: 
 - mixer6

subsets: 
- dev

num_workers: 16
sample_rate: 16000
batch_size: 1  # batch size for diarization and ASR
verbose: True # enable additional logging
output_root: ./nemo_experiments

diar_config: "chime8-baseline"  # alias of the diarization model
diar_param: "v1"  # optional tag of the diarization model
max_num_spks: 8  # maximum number of speakers in the dataset

########### Don't change ##################
diar_base_dir: ${output_root}/diar_outputs
gss_output_dir: ${output_root}/gss_outputs
asr_output_dir: ${output_root}/asr_outputs
eval_output_dir: ${output_root}/eval_outputs
prepared_manifest_vad_input: null  # automatically set by the scripts
gpu_id: 0  # set CUDA_VISIBLE_DEVICES to control which gpu to use
device: cpu  # NO NEED TO CHANGE THIS, it's used when initializing msdd, then overwritten by gpu_id
audio_type: "flac"
###########################################


optuna:
  study_name: chime8_optuna  # name of the optuna job
  storage: sqlite:///optuna-chime8.db  # data will be stored at ./optuna-chime8.db
  output_log: ./optuna-chime8.log  # where optuna log will be stored
  n_trials: 10000  # Number of trials to run with optuna
  n_jobs: -1  # Number of parallel jobs to run, set -1 to use all GPUs
  speaker_output_dir: ${output_root}  # directory for pre-computed embeddings for diarization
  save_gss_output: False  # whether to save GSS output, use False since GSS outputs are huge
  save_diar_output: False  # set to True if you have a lot of disk space
  save_asr_output: False  # set to True if you have a lot of disk space
  save_eval_output: True  # set to False if you don't have much disk space


preprocess:
  text_norm: chime7  # choices in [chime6, chime7]
  ignore_shorter: 0.2  # ignore audios shorter than this duration (in seconds)


diarizer:
  dereverberation: 
    toggle: True # True or False
    block_length_sec: 40 # process one block at a time seconds at a time
    block_overlap_sec: 2 # overlap between blocks, apply averaging
    fft_length: 1024
    hop_length: 256
    dereverb: 
      filter_length: 10
      prediction_delay: 3
      num_iterations: 10
    use_dtype: torch.cfloat # About 10x faster than torch.cdouble
  use_saved_embeddings: True # True or False
  manifest_filepath: ${diar_base_dir}/diar_manifest.json
  out_dir: ${diar_base_dir}
  speaker_out_dir: ${diar_base_dir}/speaker_outputs
  oracle_vad: False # If True, uses RTTM files provided in the manifest file to get speech activity (VAD) timestamps
  collar: 0.25 # Collar value for scoring
  ignore_overlap: False # Consider or ignore overlap segments while scoring
  
  multichannel:
    parameters:
      mix_count: 3
      power_p: 2
      channel_selector: null
      multi_channel_mode: 'mc_vad_cc_mixer' # Multichannel model: 'mc_vad', 'mc_vad_cc', 'mc_avg' and 'mc_1st_ch'. Default is "mc_vad"
      mc_max_vad: False
      mc_audio_normalize: True

  vad:
    model_path: ${vad_model_path} # .nemo local model path or pretrained VAD model name 
    external_vad_manifest: null # This option is provided to use external vad and provide its speech activity labels for speaker embeddings extraction. Only one of model_path or external_vad_manifest should be set

    parameters: # Tuned parameters for CH109 (using the 11 multi-speaker sessions as dev set) 
      shift_length_in_sec: 0.02 # Shift length in sec for generate frame level VAD prediction
      top_k_channels: 5 # If multi-channel audio files are provided, use top-k channels in terms of VAD logit values
      use_external_vad: True
      frame_vad_threshold: 0.37
      window_length_in_sec: 0.02  # Window length in sec for VAD context input 
      smoothing: "median" # False or type of smoothing method (eg: median)
      overlap: 0.5 # Overlap ratio for overlapped mean/median smoothing filter
      onset: 0.3 # Onset threshold for detecting the beginning and end of a speech 
      offset: 0.3 # Offset threshold for detecting the end of a speech
      pad_onset: 0.14 # Adding durations before each speech segment 
      pad_offset: 0.17 # Adding durations after each speech segment 
      min_duration_on: 0.4 # Threshold for small non_speech deletion
      min_duration_off: 0.55 # Threshold for short speech segment deletion
      filter_speech_first: True 

  speaker_embeddings:
    model_path: titanet_large # .nemo local model path or pretrained model name (titanet_large, ecapa_tdnn or speakerverification_speakernet)
    parameters:
      window_length_in_sec: [3.0,1.5,0.5] # Window length(s) in sec (floating-point number). either a number or a list. ex) 1.5 or [1.5,1.0,0.5]
      shift_length_in_sec: [1.5,0.75,0.25] # Shift length(s) in sec (floating-point number). either a number or a list. ex) 0.75 or [0.75,0.5,0.25]
      multiscale_weights: [2.25, 1.625, 1.0]
      interpolate_scale: 0.1
      save_embeddings: True # If True, save speaker embeddings in pickle format. This should be True if clustering result is used for other models, such as `msdd_model`.
  
  clustering:
    parameters:
      oracle_num_speakers: False # If True, use num of speakers value provided in manifest file.
      max_num_speakers: ${max_num_spks} # Max number of speakers for each recording. If an oracle number of speakers is passed, this value is ignored.
      min_num_speakers: 2 # Max number of speakers for each recording. If an oracle number of speakers is passed, this value is ignored.
      enhanced_count_thres: 80 # If the number of segments is lower than this number, enhanced speaker counting is activated.
      max_rp_threshold: 0.06
      sparse_search_volume: 25 # The higher the number, the more values will be examined with more time. 
      maj_vote_spk_count: False  # If True, take a majority vote on multiple p-values to estimate the number of speakers.
      clustering_scale_index: 2 # The index of scale to be used for clustering. 
      sync_score_thres: 0.9 # Cascaed clustering threshold for deciding whether to proceed to the finer scale.
      max_mc_ch_num: 5
      
       
  msdd_model:
    model_path: ${msdd_model_path} # .nemo local model path or pretrained model name for multiscale diarization decoder (MSDD)
    parameters:
      use_speaker_model_from_ckpt: True # If True, use speaker embedding model in checkpoint. If False, the provided speaker embedding model in config will be used.
      infer_batch_size: ${batch_size} # Batch size for MSDD inference. 
      sigmoid_threshold: [0.55] # Sigmoid threshold for generating binarized speaker labels. The smaller the more generous on detecting overlaps.
      seq_eval_mode: False # If True, use oracle number of speaker and evaluate F1 score for the given speaker sequences. Default is False.
      split_infer: True # If True, break the input audio clip to short sequences and calculate cluster average embeddings for inference.
      diar_window_length: 50 # The length of split short sequence when split_infer is True.
      global_average_mix_ratio: 0.3
      global_average_window_count: 190 # The number of windos to calculate global average embedding for both left and right. If 3, 3+3+1 = 7 windows are used.
      overlap_infer_spk_limit: 0.3 # If the proportion of speaking time is lower than this, ignore those speakers for overlap detection.
      msdd_overlap_add: False
      use_1ch_from_ch_clus: False
      mask_spks_with_clus: True
      ts_vad_threshold: -1 # if negative number, use VAD for clustering diarizer.
      infer_overlap: True
      affinity_weighting: True
      power_p_aff: 3
      thres_aff: 0.25
      use_ts_vad: True
      multi_ch_late_fusion_mode: True # True or False
      mc_late_fusion_mode: "post_mean" 
      infer_mode: "vad_masked_logits_force_1spk_rel_thres"
      system_name: ${diar_config}
      use_var_weights: False
  

gss:  
  top_k_channels: 80  # select top_k percentage of best channels from audios [0,100]
  # Args similar to the original GSS implementaion
  # See https://github.com/desh2608/gss for parameter explanations
  bss_iterations: 5
  context_duration: 15
  use_garbage_class: true
  min_segment_length: 0.0
  max_segment_length: 30
  max_batch_duration: 100
  max_batch_cuts: 1
  num_buckets: 4
  force_overwrite: false
  duration_tolerance: 3.0
  channels: null 
  torchaudio_backend: "soundfile"
  dereverb_filter_length: 5
  mc_mask_min_db: -60
  mc_postmask_min_db: -9
  dereverb_prediction_delay: 2
  dereverb_num_iterations: 3
  mc_filter_type: pmwf
  mc_filter_num_iterations: 5
  mc_filter_postfilter: "ban"


asr:
  # Normalize audio to this dB level
  normalize_db: null  

  model_path: ${asr_model_path}  # Path to a .nemo file
  pretrained_name: null  # Name of a pretrained model
  audio_dir: null  # Path to a directory which contains audio files
  dataset_manifest: null  # Path to dataset's JSON manifest or a dir containing manifests
  channel_selector: "average"  # Used to select a single channel from multichannel audio, or use `average` across channels
  audio_key: 'audio_filepath'  # Used to override the default audio key in dataset_manifest
  eval_config_yaml: null  # Path to a yaml file of config of evaluation

  # General configs
  output_filename: null
  batch_size: ${batch_size}  # use a smaller batch if beam size is large
  num_workers: ${num_workers}
  append_pred: False  # Sets mode of work, if True it will add new field transcriptions.
  pred_name_postfix: null # If you need to use another model name, rather than standard one.
  random_seed: null  # seed number going to be used in seed_everything()

  # Set to True to output greedy timestamp information (only supported models)
  compute_timestamps: False

  # Set to True to output language ID information
  compute_langs: False

  # Set `cuda` to int to define CUDA device. If 'None', will look for CUDA
  # device anyway, and do inference on CPU only if CUDA device is not found.
  # If `cuda` is a negative number, inference will be on CPU only.
  cuda: ${gpu_id}
  allow_mps: False  # allow to select MPS device (Apple Silicon M-series GPU)
  amp: False
  audio_type: "wav"  # GSS output wav files

  # Recompute model transcription, even if the output folder exists with scores.
  overwrite_transcripts: True

  # Decoding strategy for CTC models
  ctc_decoding: null

  # Decoding strategy for RNNT models
  rnnt_decoding: 
    strategy: "maes" 
    beam:
      ngram_lm_model: ${lm_model_path}
      ngram_lm_alpha: 0.0
      beam_size: 8 
      return_best_hypothesis: true
      maes_num_steps: 5
      maes_prefix_alpha: 3
      maes_expansion_gamma: 2.8
      maes_expansion_beta: 5

  # decoder type: ctc or rnnt, can be used to switch between CTC and RNNT decoder for Joint RNNT/CTC models
  decoder_type: null

  # Use this for model-specific changes before transcription
  model_change: 
    conformer: 
      # Change self_attention_model for Conformer
      # Options:
      #  'rel_pos': relative positional embedding and Transformer-XL
      #  'rel_pos_local_attn': relative positional embedding and Transformer-XL with local attention using
      #   overlapping chunks. Attention context is determined by att_context_size parameter.
      #  'abs_pos': absolute positional embedding and Transformer
      # If None is provided, self_attention_model is not changed.
      self_attention_model: null

      # Change the attention context size by providing 2 integers,
      # corresponding to left and right context, or -1 for full context.
      # If None is provided, the attention context size isn't changed.
      att_context_size: null

eval:
  dasr_root: ${chime_data_root}
  hyp_folder: null
  partition: dev
  output_folder: null
  # Whether or not use diarization to re-order the system output, if "set to false we will not re-order 
  #the system output (you can set to false if you are using oracle diarization)
  diarization: True 
  falign: false  # Path to folder containing forced-alignment rttms. (not used now)
  collar: 500  # Diarization metrics collar in ms. 500ms collar in pyannote is equivalent to 250ms start and end collar in dscore.
  # If ignore_missing=True, it will ignore missing JSON for a particular scenario, in case you want to score e.g. only DiPCo. 
  # If False, missing the corresponding JSON for a particular scenario will raise an error.
  ignore_missing: True 